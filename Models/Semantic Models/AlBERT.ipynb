{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AlBERT.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"315d400ac54d48a58867d8f3f685cc72":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_eb8a3cdb3b554fcab6a0e01087a9ad5f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a1161eae865e4cdb8fd6267e05cd665f","IPY_MODEL_39462cbc968f4a1db23042d4bda6c334"]}},"eb8a3cdb3b554fcab6a0e01087a9ad5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a1161eae865e4cdb8fd6267e05cd665f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fffc5975e52e4ed3addcc808623b8b16","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":684,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":684,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9de719b26f8c49a097002eb5f031fb0b"}},"39462cbc968f4a1db23042d4bda6c334":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a99e2e8959d04c8e93fa54bdd6c15403","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 684/684 [00:02&lt;00:00, 324B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b2e137e30d0b4502869809a050966920"}},"fffc5975e52e4ed3addcc808623b8b16":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9de719b26f8c49a097002eb5f031fb0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a99e2e8959d04c8e93fa54bdd6c15403":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b2e137e30d0b4502869809a050966920":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"053a1cbd8a284243904f836218a110dd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5ea2a0a3712d405bace574bc5cc2cf2d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ca4cfe465fea4777a61ad1b5696c9972","IPY_MODEL_4c13bea958bd4ce89a46aec88df6fca6"]}},"5ea2a0a3712d405bace574bc5cc2cf2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ca4cfe465fea4777a61ad1b5696c9972":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_61d9b4f1ebe84d5b90630f5c349f5b3e","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1882978,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1882978,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e10f2d69cbe247e3b21c7137c1f66c25"}},"4c13bea958bd4ce89a46aec88df6fca6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_586702c1d04d49aead0937842aa5e1b0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.88M/1.88M [00:01&lt;00:00, 1.33MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2ac75f101263429bb5214fa91aba213c"}},"61d9b4f1ebe84d5b90630f5c349f5b3e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e10f2d69cbe247e3b21c7137c1f66c25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"586702c1d04d49aead0937842aa5e1b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2ac75f101263429bb5214fa91aba213c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"99f70fc8b40543f0a05a435a5808b427":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2e743764325c488d8effc115f16bb2a8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_49c7b252676c4f3ebcecbbc3a750a9f0","IPY_MODEL_e23ef60224554fa6bbe8a25db4181714"]}},"2e743764325c488d8effc115f16bb2a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"49c7b252676c4f3ebcecbbc3a750a9f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7a5ed893a88e4f7aac6342b1ed8c8eff","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":73062448,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":73062448,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a0ed2eb25b4349cbbcc55cf865255f7f"}},"e23ef60224554fa6bbe8a25db4181714":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_70e4cb3e4b34461291b4c0ade3dcd38b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 73.1M/73.1M [00:02&lt;00:00, 30.6MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1aaeafb88e7945f2a267031080cab6e3"}},"7a5ed893a88e4f7aac6342b1ed8c8eff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a0ed2eb25b4349cbbcc55cf865255f7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"70e4cb3e4b34461291b4c0ade3dcd38b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1aaeafb88e7945f2a267031080cab6e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"e32zAK7_1zks"},"source":["# Import Packages and Installations"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ILnCtDwZ1ymP","executionInfo":{"status":"ok","timestamp":1613366288294,"user_tz":-210,"elapsed":33608,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"a1b88b0c-bd06-4419-ad9c-14dd015b94dc"},"source":["# Install required packages for Albert model\r\n","!pip install -q sentencepiece\r\n","!pip install -q transformers\r\n","!pip install -q tokenizers\r\n","!pip install -qU hazm\r\n","!pip install -qU clean-text[gpl]\r\n","\r\n","!mkdir resources\r\n","!wget -q \"https://github.com/sobhe/hazm/releases/download/v0.5/resources-0.5.zip\" -P resources\r\n","!unzip -qq resources/resources-0.5.zip -d resources\r\n","\r\n","!pip install faiss-cpu\r\n","\r\n","!rm -rf /content/4ccae468eb73bf6c4f4de3075ddb5336\r\n","!rm -rf /content/preproc\r\n","!rm preprocessing.py utils.py\r\n","!mkdir -p /content/preproc\r\n","!git clone https://gist.github.com/4ccae468eb73bf6c4f4de3075ddb5336.git /content/preproc/\r\n","!mv /content/preproc/* /content/\r\n","!rm -rf /content/preproc\r\n","\r\n","\r\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 1.2MB 8.6MB/s \n","\u001b[K     |████████████████████████████████| 1.8MB 7.4MB/s \n","\u001b[K     |████████████████████████████████| 3.2MB 34.7MB/s \n","\u001b[K     |████████████████████████████████| 890kB 55.9MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 317kB 9.2MB/s \n","\u001b[K     |████████████████████████████████| 235kB 15.2MB/s \n","\u001b[K     |████████████████████████████████| 1.4MB 17.5MB/s \n","\u001b[?25h  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 71kB 5.7MB/s \n","\u001b[K     |████████████████████████████████| 133kB 12.3MB/s \n","\u001b[K     |████████████████████████████████| 245kB 12.7MB/s \n","\u001b[?25h  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting faiss-cpu\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/a8/ed1601e6e94702ad691465bd1bead221dd2984f741bf384011b4dc59130e/faiss_cpu-1.7.0-cp36-cp36m-manylinux2014_x86_64.whl (8.1MB)\n","\u001b[K     |████████████████████████████████| 8.2MB 7.5MB/s \n","\u001b[?25hInstalling collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.7.0\n","rm: cannot remove 'preprocessing.py': No such file or directory\n","rm: cannot remove 'utils.py': No such file or directory\n","Cloning into '/content/preproc'...\n","remote: Enumerating objects: 7, done.\u001b[K\n","remote: Total 7 (delta 0), reused 0 (delta 0), pack-reused 7\u001b[K\n","Unpacking objects: 100% (7/7), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bjOVcC5z19xt","executionInfo":{"status":"ok","timestamp":1613366508645,"user_tz":-210,"elapsed":928,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}}},"source":["import numpy as np \r\n","import pandas as pd\r\n","import re\r\n","from tqdm import tqdm \r\n","import os\r\n","# import yake\r\n","from hazm import stopwords_list\r\n","from __future__ import unicode_literals\r\n","from hazm import *\r\n","import pickle\r\n","import requests\r\n","from termcolor import colored\r\n","from preprocessing import cleaning\r\n","\r\n","import time\r\n","\r\n","import hazm\r\n","import plotly.express as px\r\n","import plotly.graph_objects as go\r\n","from itertools import chain\r\n","# for the models\r\n","import tensorflow as tf\r\n","import matplotlib.pyplot as plt\r\n","\r\n","\r\n","# BERT base\r\n","from transformers import BertTokenizer, BertModel\r\n","import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","from __future__ import unicode_literals\r\n","import torch.nn.functional as FloatingPointError\r\n","\r\n","import faiss\r\n","# Albert\r\n","from transformers import AutoConfig, AutoTokenizer, AutoModel\r\n","from transformers import TFAlbertModel\r\n","\r\n","Albert_path = \"m3hrdadfi/albert-fa-base-v2\"\r\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"rYHZrtkM2G0p"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4oYPBOzi2IyC"},"source":["# Loading the dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1prnheXz2KKB","executionInfo":{"status":"ok","timestamp":1613366330181,"user_tz":-210,"elapsed":34726,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"addae82c-9006-48e6-e0c9-1fd12da25e3e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","data_address = '/content/drive/MyDrive/COVID-PSS.xls'\n","keys_address = '/content/drive/MyDrive/keywords_final_distilled_NE (1).pickle'\n","cleaned_titles_address = '/content/drive/MyDrive/title_cleaned_without_corona_2.pkl'\n","\n","\n","df = pd.read_csv(data_address)\n","list_t = pd.read_pickle(cleaned_titles_address)\n","\n","keywords = pd.read_pickle(keys_address)\n","keywords = [v for k,v in keywords.items()]\n","\n","\n","\n","assert len(keywords) == len(df)\n","df['keywords'] = keywords\n","df.drop(columns=['img', 'link'], inplace=True)\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GKfeFHFg2gtK","executionInfo":{"status":"ok","timestamp":1613366330184,"user_tz":-210,"elapsed":34727,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}}},"source":["# preparing inputs for semantic\r\n","\r\n","corpora = []\r\n","for i in range(len(list_t)):\r\n","\r\n","    keys = '[SEP]'.join(keywords[i])\r\n","    corpora.append(' '.join([list_t[i], keys]))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8lXvwPUW2ifX"},"source":["# Helpers"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3mfhl4QD2grL","executionInfo":{"status":"ok","timestamp":1613367400944,"user_tz":-210,"elapsed":1051,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"0e1cd264-c3a7-4f94-e9a4-bf9022e71672"},"source":["#-------------------getting some description----------#\r\n","\r\n","def tokenized_info(corpora, tokenizer, config):\r\n","    \"\"\"\r\n","    Gets the corpus and outputs the info related to the number of tokens\r\n","    of all records\r\n","    \"\"\"\r\n","\r\n","    print(f'Total number of records: {len(corpora)}')\r\n","\r\n","    tokenized_corpora_lengths = [len(tokenizer.tokenize(corp)) for corp in corpora]\r\n","    max_, min_, avg_ = max(tokenized_corpora_lengths),\\\r\n","                    min(tokenized_corpora_lengths),\\\r\n","                    np.ceil(np.mean(tokenized_corpora_lengths))\r\n","\r\n","    print(colored('The maximum length: ', 'red'), max_)\r\n","    print(colored('The minimum length: ', 'green'), min_)\r\n","    print(colored('The average length: ', 'white'), avg_)\r\n","\r\n","\r\n","    allowed_len = config.max_position_embeddings\r\n","    not_allowed = len([i for i in tokenized_corpora_lengths if i>allowed_len])\r\n","    print('In total ', colored(not_allowed, 'blue'), f' records\\nare longer than the max_len_seq wich is {allowed_len}')\r\n","\r\n","\r\n","    # --------------------- The Encoder------------------#\r\n","\r\n","def create_input_batches(corpora, tokenizer, \r\n","                         batch_size=128, max_len=512):\r\n","    \"\"\"\r\n","    Gets the corpora and outputs a number of batches with \r\n","    input ids\r\n","    attention masks\r\n","    token type ids\r\n","\r\n","    For the semantic search we only get the first two\r\n","    \"\"\"\r\n","\r\n","    all_inputs = {}\r\n","\r\n","    for i in tqdm(range(0, len(corpora), batch_size),\r\n","                  position=0, leave=True):\r\n","\r\n","        tokens = tokenizer.batch_encode_plus(\r\n","            corpora[i:i+batch_size],\r\n","            padding='max_length',\r\n","            truncation = True,\r\n","            max_length = max_len,\r\n","            add_special_tokens = True,\r\n","            pad_to_max_length=True,\r\n","            )\r\n","        \r\n","        all_inputs['Batch_'+ str(int(i/batch_size))] = tokens\r\n","    print('\\nTotal number of batches: ', len(all_inputs))\r\n","    return all_inputs\r\n","\r\n","\r\n","# ------------------------ Mean pooling on GPU--------------\r\n","\r\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n","print('\\nworking on', device)\r\n","    \r\n","def get_embeddings(all_inputs, model):\r\n","    \"\"\"\r\n","    gets batches of input and outputs the mean of all tokens in a sentence\r\n","    which has 768 elements for each sentence.\r\n","    first we turn each required input batch into a tensor\r\n","    then give it to the model\r\n","    and get the first of all hidden states from it\r\n","    then we add all to mean_of_all_batches\r\n","\r\n","    shape of the output:\r\n","\r\n","    [# layers, # batches, # tokens, # features]\r\n","    \"\"\"\r\n","    \r\n","    model_gpu = model.to(device)\r\n","    mean_of_all_batches = []\r\n","\r\n","    for i in tqdm(range(len(all_inputs)), leave = True, position = 0):\r\n","        #print(f'Batch {i}')\r\n","        input_ids_batch = torch.tensor(all_inputs['Batch_'+ str(i)].input_ids)\r\n","        attention_masks_batch = torch.tensor(all_inputs['Batch_'+ str(i)].attention_mask)\r\n","\r\n","        input_ids_d = input_ids_batch.to(device)\r\n","        masks_d = attention_masks_batch.to(device)\r\n","\r\n","        with torch.no_grad(): \r\n","            # print('went into no grad')\r\n","            outputs = model_gpu(input_ids_d, masks_d)  \r\n","            # print('went into the model.')\r\n","            hidden_states = outputs[2][0]\r\n","            # print('got the hidden states')\r\n","    # mean_i = torch.mean(hidden_states[0], 0)\r\n","\r\n","\r\n","        means_for_batch_i = []\r\n","        for j in range(len(hidden_states)):\r\n","            mean_j = torch.mean(hidden_states[j], 0)\r\n","            means_for_batch_i.append(mean_j)\r\n","\r\n","        mean_of_all_batches.append(means_for_batch_i)\r\n","\r\n","    mean_of_all_batches = list(chain.from_iterable(mean_of_all_batches))\r\n","    print('\\nTotal number of sentences: ', len(mean_of_all_batches))\r\n","    return mean_of_all_batches\r\n","    # return mean_i\r\n","\r\n","\r\n","# --------------------------- query 🤔--------------------#\r\n","\r\n","\r\n","def get_query_embeddings(question, tokenizer,\r\n","                         model, max_len=512):\r\n","\r\n","    tokenized = tokenizer.encode_plus(\r\n","        question,\r\n","        padding='max_length',\r\n","        truncation = True,\r\n","        max_length = max_len,\r\n","        add_special_tokens = True,\r\n","        pad_to_max_length=True,\r\n","        )\r\n","    tokens_tensor = torch.tensor([tokenized.input_ids])\r\n","    attention_masks = torch.tensor([tokenized.attention_mask])\r\n","    \r\n","    input_ids_d = tokens_tensor.to(device)\r\n","    masks_d = attention_masks.to(device)\r\n","    with torch.no_grad(): \r\n","        outputs = model(input_ids_d, masks_d)  \r\n","        hidden_states = outputs[2][0]\r\n","\r\n","    mean_i = torch.mean(hidden_states[0], 0)\r\n","    return mean_i\r\n","\r\n","\r\n","\r\n","#--------------------------FAISS cosine sim for one example--------------------#\r\n","\r\n","def get_FAISS_cosine_results(sent_embs, question, tokenizer,\r\n","                     model, max_len=512, top_n = 10,\r\n","                      print_results=True ):\r\n","    \r\n","    query_embeddings = get_query_embeddings(question, tokenizer,\r\n","                         model, max_len=512)\r\n","    \r\n","    index = faiss.index_factory(768, \"Flat\", faiss.METRIC_INNER_PRODUCT)\r\n","\r\n","    emb_cor = np.array([i.cpu().numpy() for i in sent_embs])\r\n","    faiss.normalize_L2(emb_cor)\r\n","    index.add(emb_cor)\r\n","\r\n","    emb_que = np.array([query_embeddings.cpu().numpy()])\r\n","    faiss.normalize_L2(emb_que)\r\n","\r\n","    top_k = index.search(emb_que, top_n)\r\n","\r\n","    matches = []\r\n","    if print_results == True:\r\n","\r\n","        print('Top results!')\r\n","        n = 0\r\n","        for score, idx in zip(top_k[0][0], top_k[1][0]):\r\n","            print(colored(f'{n}: {idx}th', 'blue'),' corpus with score', colored( f'{score:.5f}:\\n', 'blue'),  corpora[idx])\r\n","            matches.append(idx)\r\n","            n+=1\r\n","\r\n","    return matches\r\n","\r\n","\r\n","\r\n","#--------------------------FAISS L2 distance for one example--------------------#\r\n","\r\n","def get_FAISS_L2_results(sent_embs, question, tokenizer,\r\n","                     model, max_len=512, top_n = 10,\r\n","                      print_results=True ):\r\n","    \r\n","    query_embeddings = get_query_embeddings(question, tokenizer,\r\n","                         model, max_len=512)\r\n","    \r\n","\r\n","    emb_cor = np.array([i.cpu().numpy() for i in sent_embs])\r\n","    emb_que = np.array([query_embeddings.cpu().numpy()])\r\n","\r\n","\r\n","    index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\r\n","\r\n","    index.add_with_ids(np.array(emb_cor), np.array(range(0, len(emb_cor))))\r\n","\r\n","    faiss.write_index(index, 'corona_corpora')\r\n","    index = faiss.read_index('corona_corpora')\r\n","\r\n","    top_k = index.search(emb_que, top_n)\r\n","\r\n","    matches = []\r\n","    if print_results == True:\r\n","\r\n","        n = 0\r\n","        for score, idx in zip(top_k[0][0], top_k[1][0]):\r\n","            print(colored(f'{n}: {idx}th', 'blue'),' corpus with score', colored( f'{score:.5f}:\\n', 'blue'),  corpora[idx])\r\n","            matches.append(idx)\r\n","            n+=1\r\n","\r\n","    return matches\r\n","\r\n","\r\n","    \r\n","# phrases that need to be removed from titles\r\n","corona_phrases = ['کرونایی', 'کروناست' ,'کرونا', 'شیوع', 'بحران', 'ویروس',\r\n","                  'ویروس جدید', 'coronavirus', 'corona', 'کووید-19 ', \r\n","                  'کووید', 'بیماری', 'بیمارانی', 'بیماران', '-۱۹', ' وی ', '19', '۱۹',\r\n","                  ' بیمار ', 'كرونا', 'كوويد', 'ويروس', r'(\\s+)',]\r\n","\r\n","\r\n","normalizer = hazm.Normalizer()\r\n","\r\n","def clean(text):\r\n","    \"\"\"Cleans the titles for the semantic models\"\"\"\r\n","    for pattern in corona_phrases:\r\n","        text = re.sub(pattern, \" \", text)\r\n","\r\n","    text = re.sub(' +[\\w] +', \" \", text)\r\n","    text = normalizer.normalize(text)\r\n","\r\n","    return text\r\n","\r\n","\r\n","#---------------------------------- get the batch results for this model-----------------------#\r\n","\r\n","def get_resutls(questions, tokenizer, model, top_n):\r\n","    \r\n","    results = []\r\n","    i=0\r\n","    for question in questions:\r\n","\r\n","        print(colored(f'{i}: ', 'blue'), question)\r\n","        # print('question type', type(question))\r\n","        i+=1\r\n","\r\n","        # we give the cleaned question to the semantic model \r\n","        question_cleaned = clean(question)\r\n","\r\n","        question_emb = get_query_embeddings(question_cleaned, tokenizer,\r\n","                         model, max_len=512)\r\n","        \r\n","        emb_que = np.array([question_emb.cpu().numpy()])\r\n","        faiss.normalize_L2(emb_que)\r\n","\r\n","        top_k = index.search(emb_que, top_n)\r\n","        indices = []\r\n","        scores = []\r\n","\r\n","        # saving all the reults in a dictionary\r\n","        for score, idx in zip(top_k[0][0], top_k[1][0]):\r\n","            \r\n","            indices.append(idx)\r\n","            scores.append(score)\r\n","\r\n","        results.append({'question':question,\r\n","                        'index':indices,\r\n","                        'score':scores})\r\n","    return results"],"execution_count":25,"outputs":[{"output_type":"stream","text":["\n","working on cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mZpvboRY2qPk"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"ennVCrsQ2go6","colab":{"base_uri":"https://localhost:8080/","height":178,"referenced_widgets":["315d400ac54d48a58867d8f3f685cc72","eb8a3cdb3b554fcab6a0e01087a9ad5f","a1161eae865e4cdb8fd6267e05cd665f","39462cbc968f4a1db23042d4bda6c334","fffc5975e52e4ed3addcc808623b8b16","9de719b26f8c49a097002eb5f031fb0b","a99e2e8959d04c8e93fa54bdd6c15403","b2e137e30d0b4502869809a050966920","053a1cbd8a284243904f836218a110dd","5ea2a0a3712d405bace574bc5cc2cf2d","ca4cfe465fea4777a61ad1b5696c9972","4c13bea958bd4ce89a46aec88df6fca6","61d9b4f1ebe84d5b90630f5c349f5b3e","e10f2d69cbe247e3b21c7137c1f66c25","586702c1d04d49aead0937842aa5e1b0","2ac75f101263429bb5214fa91aba213c","99f70fc8b40543f0a05a435a5808b427","2e743764325c488d8effc115f16bb2a8","49c7b252676c4f3ebcecbbc3a750a9f0","e23ef60224554fa6bbe8a25db4181714","7a5ed893a88e4f7aac6342b1ed8c8eff","a0ed2eb25b4349cbbcc55cf865255f7f","70e4cb3e4b34461291b4c0ade3dcd38b","1aaeafb88e7945f2a267031080cab6e3"]},"executionInfo":{"status":"ok","timestamp":1613366524109,"user_tz":-210,"elapsed":7346,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"1ff185b1-cfae-4947-98e5-9410254fc622"},"source":["albert_config = AutoConfig.from_pretrained(Albert_path)\r\n","albert_tokenizer = AutoTokenizer.from_pretrained(Albert_path)\r\n","albert_model = AutoModel.from_pretrained(Albert_path, \r\n","                                 output_hidden_states = True)"],"execution_count":8,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"315d400ac54d48a58867d8f3f685cc72","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=684.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"053a1cbd8a284243904f836218a110dd","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1882978.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"99f70fc8b40543f0a05a435a5808b427","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=73062448.0, style=ProgressStyle(descrip…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QPyaEMGo2gmx","executionInfo":{"status":"ok","timestamp":1613366529334,"user_tz":-210,"elapsed":900,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"3793b413-5861-4509-b4af-7427c7d10a4f"},"source":["albert_model.eval()\r\n","print('Model is set on the evaluation mode.')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Model is set on the evaluation mode.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"30PnKT5e2gky","executionInfo":{"status":"ok","timestamp":1613366535183,"user_tz":-210,"elapsed":3302,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"bb217c8f-021f-483e-c61a-68a022d4b9a0"},"source":["tokenized_info(corpora, albert_tokenizer, albert_config)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Total number of records: 3536\n","\u001b[31mThe maximum length: \u001b[0m 1888\n","\u001b[32mThe minimum length: \u001b[0m 11\n","\u001b[37mThe average length: \u001b[0m 135.0\n","In total  \u001b[34m98\u001b[0m  records\n","are longer than the max_len_seq wich is 512\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lF8q63Kc2giq","executionInfo":{"status":"ok","timestamp":1613366538627,"user_tz":-210,"elapsed":2605,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"5ffb8aa1-4431-4a14-93ea-2c944961a8bc"},"source":["all_inputs_albert = create_input_batches(corpora, albert_tokenizer, \r\n","                         batch_size=128, max_len=512)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["100%|██████████| 28/28 [00:01<00:00, 16.41it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Total number of batches:  28\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"IYN9db7z2ggi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613366732387,"user_tz":-210,"elapsed":165402,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"3356f5b4-6c33-46c1-b082-eb0ac205bb93"},"source":["mean_of_all_sentences_albert = get_embeddings(all_inputs_albert, model=albert_model)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["100%|██████████| 28/28 [02:34<00:00,  5.53s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","Total number of sentences:  3536\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"SxVdsjru25xd"},"source":["# Single Qustion"]},{"cell_type":"code","metadata":{"id":"WHiynr_Z2geZ"},"source":["question = 'واکسن تا چه حد موثر است'\r\n","\r\n","albert_indices = get_FAISS_cosine_results(sent_embs = mean_of_all_sentences_albert,\r\n","                         question= question,\r\n","                         tokenizer=albert_tokenizer,\r\n","                         model=albert_model,\r\n","                         max_len=512,\r\n","                         top_n = 50,\r\n","                         print_results=True )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9f2y2TEJJEh7"},"source":["# In Batches"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N6l4oiwhJGfC","executionInfo":{"status":"ok","timestamp":1613367406163,"user_tz":-210,"elapsed":3224,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"3c546382-2300-400e-ea05-8631115c546c"},"source":["questions = pd.read_pickle('/content/drive/MyDrive/CoPer paper-Models/Sample Queries/Titles_with_Corona.pkl')\r\n","\r\n","\r\n","index = faiss.index_factory(768, \"Flat\", faiss.METRIC_INNER_PRODUCT)\r\n","emb_cor = np.array([i.cpu().numpy() for i in mean_of_all_sentences_albert])\r\n","faiss.normalize_L2(emb_cor)\r\n","index.add(emb_cor)\r\n","\r\n","\r\n","results = get_resutls(questions, tokenizer=albert_tokenizer, model=albert_model, top_n=50)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["\u001b[34m0: \u001b[0m دردسر وکلا با کرونابحران کرونا دادگاه‌‌ها را تعطیل می‌ کند\n","\u001b[34m1: \u001b[0m ٢٢٨٢ ابتلا ۵۷ فوتی جدید کرونا در کشور خوزستان همچنان در وضعیت قرمز\n","\u001b[34m2: \u001b[0m رفتار عجیب ویروس کرونا از طریق افرادی که علائم ندارند منتقل می‌شود\n","\u001b[34m3: \u001b[0m بهبودیافتگان کرونا ۲۸ روز بعد از بهبودی پلاسمای خود را اهدا کنند\n","\u001b[34m4: \u001b[0m ادامه روند کاهشی فوتی‌های کرونااز سفر به چهار استان خودداری کنید\n","\u001b[34m5: \u001b[0m عملکرد برنامه کشوری غربالگری بیماری کم کاری تیروئید در نوزادان در پاندمی کرونا\n","\u001b[34m6: \u001b[0m تداوم تعطیلی صنوف پرریسک در پایتخت پلمب قهوه‌خانه به‌خاطر بی‌توجهی به کرونا\n","\u001b[34m7: \u001b[0m روند صعودی کرونا در استان سطح بالای ابتلا مرگ در کل کشور\n","\u001b[34m8: \u001b[0m در ایران مطلقا از کیت‌های صادر شده به آلمان برای تشخیص کرونا استفاده نمی‌کنیم\n","\u001b[34m9: \u001b[0m ۳۲۱ فوتی جدید کرونا در کشورشمار قربانیان از مرز ۵۰هزار تن گذشت\n","\u001b[34m10: \u001b[0m وضعیت هشدار کرونا در کرمانشاه ماسک بزنید بی‌خیال سفر شوید\n","\u001b[34m11: \u001b[0m پیشرفت در تولید واکسن کرونا در آکسفورد ۱۰ هزار نفر دیگر نیز واکسینه می‌شوند\n","\u001b[34m12: \u001b[0m ابتلای نفر از بیماران نادر به کرونا درخواست از مسئولان برای تامین دارو سر موقع\n","\u001b[34m13: \u001b[0m سند سبقت خزنده ترسناک مرگ میر کرونا در ایران از جهان\n","\u001b[34m14: \u001b[0m مصرف خودسرانه‌ مکمل‌ها ویتامین‌ها ممنوع زنجبیل لیموترش در درمان ویروس کرونا تاثیر ندارد\n","\u001b[34m15: \u001b[0m اعمال محدویت‌ها مهم‌ترین عامل کنترل کروناکدام بیماران باید به بیمارستان مراجعه کنند\n","\u001b[34m16: \u001b[0m بستری مرگ میر ناشی از کرونا در قم بالاتر از میانگین کشوری\n","\u001b[34m17: \u001b[0m زالی تاکید کرد ارتباط آلودگی هوا با مرگ میر ناشی از کرونا\n","\u001b[34m18: \u001b[0m درخواست جنجالی ستاد ملی کرونا از تلویزیون تصاویر مجلس را پخش نکنید بدآموزی دارد\n","\u001b[34m19: \u001b[0m خروج میلیون نفر از ۱۳ استان کشور۲۴۰۰ نفر علائمی از کرونا داشتند\n","\u001b[34m20: \u001b[0m یک محقق سوئدی مبتلایان به کرونا شش ماه در برابر این بیماری مصونیت دارند\n","\u001b[34m21: \u001b[0m رئیس مجلس فردا در جلسۀ سران قوا مسائل مربوط به مدیریت کرونا را مطرح خواهم کرد\n","\u001b[34m22: \u001b[0m پیش بینی افزایش فوتی‌های کرونا بازگشایی‌ها در چه صورت متوقف شود\n","\u001b[34m23: \u001b[0m ابتلای ۴۰ نفر به کرونا در یک عروسی در چارمحال بختیاری\n","\u001b[34m24: \u001b[0m به مردم بگویید قبل از خروج از ووهان تست کرونا داده‌ایم در سلامت کامل هستیم\n","\u001b[34m25: \u001b[0m قلب هایی که به خاطر کرونا می‌ایستندهشدار به بیماران قلبی\n","\u001b[34m26: \u001b[0m ۲۳۶۹ ابتلا ۷۵ فوتی جدید کرونا در کشور استان‌ در وضعیت قرمز\n","\u001b[34m27: \u001b[0m توزیع ۸۸۰ میلیاردتومان فوق العاده ویژه در بین کارکنان بهداشت ودرمان روند ابتلا بستری فوتی بر اثر کرونا در کشور نزولی است\n","\u001b[34m28: \u001b[0m داروهای کرونا از طریق پزشکان بیمارستان‌ها در اختیار داروفروشان ناصرخسرو قرار گرفته\n","\u001b[34m29: \u001b[0m ۴۵۳ فوتی جدید کرونا در کشور ۵۸۱۲ تن در وضعیت شدید بیماری\n","\u001b[34m30: \u001b[0m چرا دنیا به زدن ماسک برای مقابله با کرونا تاکید دارد ۱۰ دلیل قانع کننده\n","\u001b[34m31: \u001b[0m شمار قربانیان روزانه کرونا در ایران به ۲۲۱ تن رسید۱۹ استان همچنان در شرایط قرمز هشدار\n","\u001b[34m32: \u001b[0m خطر شیوع کرونا در مترو از مجالس عروسی عزا بیشتر است رعایت فاصله اجتماعی مهم‌تر از استفاده از ماسک\n","\u001b[34m33: \u001b[0m ۲۰۰ جان‌باخته کرونا در ۲۴ ساعت گذشته ۱۹ استان در شرایط قرمز هشدار\n","\u001b[34m34: \u001b[0m افزایش شمار مبتلایان به کرونا در کشور به ۲۹۲۲مرگ ۹۲ نفر\n","\u001b[34m35: \u001b[0m ۲۵۰۰ ابتلا ۱۹۸ فوتی جدید کرونا در کشور افزایش موارد بستری در بیمارستانها\n","\u001b[34m36: \u001b[0m افزایش تعداد مسافران اتوبوسفوت اتوبوسران بر اثر ابتلا به کرونا\n","\u001b[34m37: \u001b[0m پاسخ به کسانی که می‌پرسند کی از شر کرونا خلاص می‌شویم\n","\u001b[34m38: \u001b[0m نه پول خرید واکسن کرونا را داریم نه به دلیل تحریم‌ها به ما میدهند\n","\u001b[34m39: \u001b[0m ردیابی تلفن همراه کرونا مثبتها نقض قرنطینه از سوی ۷۳ درصد مبتلایان\n","\u001b[34m40: \u001b[0m رکورد تهران در فوتی‌های کرونا احتمال اعمال محدودیت در سفر به شمال\n","\u001b[34m41: \u001b[0m هیچ‌کسی در ایران به کرونا مبتلا نشده حال ۵۰ دانشجوی وارد شده از چین خوب است\n","\u001b[34m42: \u001b[0m ابتلا به کرونا در بین جوانان بیشتر شده است پیش‌بینی نقاهتگاه برای بیماران\n","\u001b[34m43: \u001b[0m کدام گروهها در خط مقابله با بحران کرونا هستندخط شکنان را بشناسیم\n","\u001b[34m44: \u001b[0m شنبه دلهره آور کرونا از راه رسیدمبادا پشیمان شویم\n","\u001b[34m45: \u001b[0m فوت ۱۸۳ بیمار کرونایی در کشور در شبانه روز گذشته ۱۰ استان قرمز\n","\u001b[34m46: \u001b[0m افزایش شمار مبتلایان کرونا به ۱۲۰هزار تنلرستان در وضعیت هشدارخوزستان همچنان قرمز\n","\u001b[34m47: \u001b[0m کاهش مقاومت بدن در برابر کرونا با زیاده روی در مصرف ویتامین‌ها مکمل‌ها\n","\u001b[34m48: \u001b[0m عبور دوباره بیماران جدید کرونا از مرز ۳۰۰۰ نفر۶۴ تن جان باختند\n","\u001b[34m49: \u001b[0m بیماران پروانه‌ای از کرونا در امان ماندند موارد ابتلا به تعداد انگشتان دست است\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pr0jCVPcJGch"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9NADhPDJGY6"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uk8v4DUC9TMU"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ot3RkgBALsBK"},"source":["# Get the results"]},{"cell_type":"code","metadata":{"id":"kvYeJ15J9TJ2","executionInfo":{"status":"ok","timestamp":1613367316111,"user_tz":-210,"elapsed":910,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}}},"source":["with open('/content/drive/MyDrive/CoPer paper-Models/Results/AlBert.pkl', 'wb') as f:\r\n","    pickle.dump(results, f)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"3CtDaXMN9TFU"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qGfMSKAN9TDE"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBdpPSF29TA8"},"source":[],"execution_count":null,"outputs":[]}]}