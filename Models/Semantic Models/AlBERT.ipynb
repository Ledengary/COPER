{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AlBERT.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"315d400ac54d48a58867d8f3f685cc72":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_eb8a3cdb3b554fcab6a0e01087a9ad5f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a1161eae865e4cdb8fd6267e05cd665f","IPY_MODEL_39462cbc968f4a1db23042d4bda6c334"]}},"eb8a3cdb3b554fcab6a0e01087a9ad5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a1161eae865e4cdb8fd6267e05cd665f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fffc5975e52e4ed3addcc808623b8b16","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":684,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":684,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9de719b26f8c49a097002eb5f031fb0b"}},"39462cbc968f4a1db23042d4bda6c334":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a99e2e8959d04c8e93fa54bdd6c15403","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 684/684 [00:02&lt;00:00, 324B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b2e137e30d0b4502869809a050966920"}},"fffc5975e52e4ed3addcc808623b8b16":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9de719b26f8c49a097002eb5f031fb0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a99e2e8959d04c8e93fa54bdd6c15403":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b2e137e30d0b4502869809a050966920":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"053a1cbd8a284243904f836218a110dd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5ea2a0a3712d405bace574bc5cc2cf2d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ca4cfe465fea4777a61ad1b5696c9972","IPY_MODEL_4c13bea958bd4ce89a46aec88df6fca6"]}},"5ea2a0a3712d405bace574bc5cc2cf2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ca4cfe465fea4777a61ad1b5696c9972":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_61d9b4f1ebe84d5b90630f5c349f5b3e","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1882978,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1882978,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e10f2d69cbe247e3b21c7137c1f66c25"}},"4c13bea958bd4ce89a46aec88df6fca6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_586702c1d04d49aead0937842aa5e1b0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.88M/1.88M [00:01&lt;00:00, 1.33MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2ac75f101263429bb5214fa91aba213c"}},"61d9b4f1ebe84d5b90630f5c349f5b3e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e10f2d69cbe247e3b21c7137c1f66c25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"586702c1d04d49aead0937842aa5e1b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2ac75f101263429bb5214fa91aba213c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"99f70fc8b40543f0a05a435a5808b427":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2e743764325c488d8effc115f16bb2a8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_49c7b252676c4f3ebcecbbc3a750a9f0","IPY_MODEL_e23ef60224554fa6bbe8a25db4181714"]}},"2e743764325c488d8effc115f16bb2a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"49c7b252676c4f3ebcecbbc3a750a9f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7a5ed893a88e4f7aac6342b1ed8c8eff","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":73062448,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":73062448,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a0ed2eb25b4349cbbcc55cf865255f7f"}},"e23ef60224554fa6bbe8a25db4181714":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_70e4cb3e4b34461291b4c0ade3dcd38b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 73.1M/73.1M [00:02&lt;00:00, 30.6MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1aaeafb88e7945f2a267031080cab6e3"}},"7a5ed893a88e4f7aac6342b1ed8c8eff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a0ed2eb25b4349cbbcc55cf865255f7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"70e4cb3e4b34461291b4c0ade3dcd38b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1aaeafb88e7945f2a267031080cab6e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"e32zAK7_1zks"},"source":["# Import Packages and Installations"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ILnCtDwZ1ymP","executionInfo":{"status":"ok","timestamp":1613366288294,"user_tz":-210,"elapsed":33608,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"a1b88b0c-bd06-4419-ad9c-14dd015b94dc"},"source":["# Install required packages for Albert model\r\n","!pip install -q sentencepiece\r\n","!pip install -q transformers\r\n","!pip install -q tokenizers\r\n","!pip install -qU hazm\r\n","!pip install -qU clean-text[gpl]\r\n","\r\n","!mkdir resources\r\n","!wget -q \"https://github.com/sobhe/hazm/releases/download/v0.5/resources-0.5.zip\" -P resources\r\n","!unzip -qq resources/resources-0.5.zip -d resources\r\n","\r\n","!pip install faiss-cpu\r\n","\r\n","!rm -rf /content/4ccae468eb73bf6c4f4de3075ddb5336\r\n","!rm -rf /content/preproc\r\n","!rm preprocessing.py utils.py\r\n","!mkdir -p /content/preproc\r\n","!git clone https://gist.github.com/4ccae468eb73bf6c4f4de3075ddb5336.git /content/preproc/\r\n","!mv /content/preproc/* /content/\r\n","!rm -rf /content/preproc\r\n","\r\n","\r\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 8.6MB/s \n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8MB 7.4MB/s \n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2MB 34.7MB/s \n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 55.9MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 317kB 9.2MB/s \n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235kB 15.2MB/s \n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4MB 17.5MB/s \n","\u001b[?25h  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 5.7MB/s \n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 12.3MB/s \n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245kB 12.7MB/s \n","\u001b[?25h  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting faiss-cpu\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/a8/ed1601e6e94702ad691465bd1bead221dd2984f741bf384011b4dc59130e/faiss_cpu-1.7.0-cp36-cp36m-manylinux2014_x86_64.whl (8.1MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.2MB 7.5MB/s \n","\u001b[?25hInstalling collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.7.0\n","rm: cannot remove 'preprocessing.py': No such file or directory\n","rm: cannot remove 'utils.py': No such file or directory\n","Cloning into '/content/preproc'...\n","remote: Enumerating objects: 7, done.\u001b[K\n","remote: Total 7 (delta 0), reused 0 (delta 0), pack-reused 7\u001b[K\n","Unpacking objects: 100% (7/7), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bjOVcC5z19xt","executionInfo":{"status":"ok","timestamp":1613366508645,"user_tz":-210,"elapsed":928,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}}},"source":["import numpy as np \r\n","import pandas as pd\r\n","import re\r\n","from tqdm import tqdm \r\n","import os\r\n","# import yake\r\n","from hazm import stopwords_list\r\n","from __future__ import unicode_literals\r\n","from hazm import *\r\n","import pickle\r\n","import requests\r\n","from termcolor import colored\r\n","from preprocessing import cleaning\r\n","\r\n","import time\r\n","\r\n","import hazm\r\n","import plotly.express as px\r\n","import plotly.graph_objects as go\r\n","from itertools import chain\r\n","# for the models\r\n","import tensorflow as tf\r\n","import matplotlib.pyplot as plt\r\n","\r\n","\r\n","# BERT base\r\n","from transformers import BertTokenizer, BertModel\r\n","import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","from __future__ import unicode_literals\r\n","import torch.nn.functional as FloatingPointError\r\n","\r\n","import faiss\r\n","# Albert\r\n","from transformers import AutoConfig, AutoTokenizer, AutoModel\r\n","from transformers import TFAlbertModel\r\n","\r\n","Albert_path = \"m3hrdadfi/albert-fa-base-v2\"\r\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"rYHZrtkM2G0p"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4oYPBOzi2IyC"},"source":["# Loading the dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1prnheXz2KKB","executionInfo":{"status":"ok","timestamp":1613366330181,"user_tz":-210,"elapsed":34726,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"addae82c-9006-48e6-e0c9-1fd12da25e3e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","data_address = '/content/drive/MyDrive/COVID-PSS.xls'\n","keys_address = '/content/drive/MyDrive/keywords_final_distilled_NE (1).pickle'\n","cleaned_titles_address = '/content/drive/MyDrive/title_cleaned_without_corona_2.pkl'\n","\n","\n","df = pd.read_csv(data_address)\n","list_t = pd.read_pickle(cleaned_titles_address)\n","\n","keywords = pd.read_pickle(keys_address)\n","keywords = [v for k,v in keywords.items()]\n","\n","\n","\n","assert len(keywords) == len(df)\n","df['keywords'] = keywords\n","df.drop(columns=['img', 'link'], inplace=True)\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GKfeFHFg2gtK","executionInfo":{"status":"ok","timestamp":1613366330184,"user_tz":-210,"elapsed":34727,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}}},"source":["# preparing inputs for semantic\r\n","\r\n","corpora = []\r\n","for i in range(len(list_t)):\r\n","\r\n","    keys = '[SEP]'.join(keywords[i])\r\n","    corpora.append(' '.join([list_t[i], keys]))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8lXvwPUW2ifX"},"source":["# Helpers"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3mfhl4QD2grL","executionInfo":{"status":"ok","timestamp":1613367400944,"user_tz":-210,"elapsed":1051,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"0e1cd264-c3a7-4f94-e9a4-bf9022e71672"},"source":["#-------------------getting some description----------#\r\n","\r\n","def tokenized_info(corpora, tokenizer, config):\r\n","    \"\"\"\r\n","    Gets the corpus and outputs the info related to the number of tokens\r\n","    of all records\r\n","    \"\"\"\r\n","\r\n","    print(f'Total number of records: {len(corpora)}')\r\n","\r\n","    tokenized_corpora_lengths = [len(tokenizer.tokenize(corp)) for corp in corpora]\r\n","    max_, min_, avg_ = max(tokenized_corpora_lengths),\\\r\n","                    min(tokenized_corpora_lengths),\\\r\n","                    np.ceil(np.mean(tokenized_corpora_lengths))\r\n","\r\n","    print(colored('The maximum length: ', 'red'), max_)\r\n","    print(colored('The minimum length: ', 'green'), min_)\r\n","    print(colored('The average length: ', 'white'), avg_)\r\n","\r\n","\r\n","    allowed_len = config.max_position_embeddings\r\n","    not_allowed = len([i for i in tokenized_corpora_lengths if i>allowed_len])\r\n","    print('In total ', colored(not_allowed, 'blue'), f' records\\nare longer than the max_len_seq wich is {allowed_len}')\r\n","\r\n","\r\n","    # --------------------- The Encoder------------------#\r\n","\r\n","def create_input_batches(corpora, tokenizer, \r\n","                         batch_size=128, max_len=512):\r\n","    \"\"\"\r\n","    Gets the corpora and outputs a number of batches with \r\n","    input ids\r\n","    attention masks\r\n","    token type ids\r\n","\r\n","    For the semantic search we only get the first two\r\n","    \"\"\"\r\n","\r\n","    all_inputs = {}\r\n","\r\n","    for i in tqdm(range(0, len(corpora), batch_size),\r\n","                  position=0, leave=True):\r\n","\r\n","        tokens = tokenizer.batch_encode_plus(\r\n","            corpora[i:i+batch_size],\r\n","            padding='max_length',\r\n","            truncation = True,\r\n","            max_length = max_len,\r\n","            add_special_tokens = True,\r\n","            pad_to_max_length=True,\r\n","            )\r\n","        \r\n","        all_inputs['Batch_'+ str(int(i/batch_size))] = tokens\r\n","    print('\\nTotal number of batches: ', len(all_inputs))\r\n","    return all_inputs\r\n","\r\n","\r\n","# ------------------------ Mean pooling on GPU--------------\r\n","\r\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n","print('\\nworking on', device)\r\n","    \r\n","def get_embeddings(all_inputs, model):\r\n","    \"\"\"\r\n","    gets batches of input and outputs the mean of all tokens in a sentence\r\n","    which has 768 elements for each sentence.\r\n","    first we turn each required input batch into a tensor\r\n","    then give it to the model\r\n","    and get the first of all hidden states from it\r\n","    then we add all to mean_of_all_batches\r\n","\r\n","    shape of the output:\r\n","\r\n","    [# layers, # batches, # tokens, # features]\r\n","    \"\"\"\r\n","    \r\n","    model_gpu = model.to(device)\r\n","    mean_of_all_batches = []\r\n","\r\n","    for i in tqdm(range(len(all_inputs)), leave = True, position = 0):\r\n","        #print(f'Batch {i}')\r\n","        input_ids_batch = torch.tensor(all_inputs['Batch_'+ str(i)].input_ids)\r\n","        attention_masks_batch = torch.tensor(all_inputs['Batch_'+ str(i)].attention_mask)\r\n","\r\n","        input_ids_d = input_ids_batch.to(device)\r\n","        masks_d = attention_masks_batch.to(device)\r\n","\r\n","        with torch.no_grad(): \r\n","            # print('went into no grad')\r\n","            outputs = model_gpu(input_ids_d, masks_d)  \r\n","            # print('went into the model.')\r\n","            hidden_states = outputs[2][0]\r\n","            # print('got the hidden states')\r\n","    # mean_i = torch.mean(hidden_states[0], 0)\r\n","\r\n","\r\n","        means_for_batch_i = []\r\n","        for j in range(len(hidden_states)):\r\n","            mean_j = torch.mean(hidden_states[j], 0)\r\n","            means_for_batch_i.append(mean_j)\r\n","\r\n","        mean_of_all_batches.append(means_for_batch_i)\r\n","\r\n","    mean_of_all_batches = list(chain.from_iterable(mean_of_all_batches))\r\n","    print('\\nTotal number of sentences: ', len(mean_of_all_batches))\r\n","    return mean_of_all_batches\r\n","    # return mean_i\r\n","\r\n","\r\n","# --------------------------- query ðŸ¤”--------------------#\r\n","\r\n","\r\n","def get_query_embeddings(question, tokenizer,\r\n","                         model, max_len=512):\r\n","\r\n","    tokenized = tokenizer.encode_plus(\r\n","        question,\r\n","        padding='max_length',\r\n","        truncation = True,\r\n","        max_length = max_len,\r\n","        add_special_tokens = True,\r\n","        pad_to_max_length=True,\r\n","        )\r\n","    tokens_tensor = torch.tensor([tokenized.input_ids])\r\n","    attention_masks = torch.tensor([tokenized.attention_mask])\r\n","    \r\n","    input_ids_d = tokens_tensor.to(device)\r\n","    masks_d = attention_masks.to(device)\r\n","    with torch.no_grad(): \r\n","        outputs = model(input_ids_d, masks_d)  \r\n","        hidden_states = outputs[2][0]\r\n","\r\n","    mean_i = torch.mean(hidden_states[0], 0)\r\n","    return mean_i\r\n","\r\n","\r\n","\r\n","#--------------------------FAISS cosine sim for one example--------------------#\r\n","\r\n","def get_FAISS_cosine_results(sent_embs, question, tokenizer,\r\n","                     model, max_len=512, top_n = 10,\r\n","                      print_results=True ):\r\n","    \r\n","    query_embeddings = get_query_embeddings(question, tokenizer,\r\n","                         model, max_len=512)\r\n","    \r\n","    index = faiss.index_factory(768, \"Flat\", faiss.METRIC_INNER_PRODUCT)\r\n","\r\n","    emb_cor = np.array([i.cpu().numpy() for i in sent_embs])\r\n","    faiss.normalize_L2(emb_cor)\r\n","    index.add(emb_cor)\r\n","\r\n","    emb_que = np.array([query_embeddings.cpu().numpy()])\r\n","    faiss.normalize_L2(emb_que)\r\n","\r\n","    top_k = index.search(emb_que, top_n)\r\n","\r\n","    matches = []\r\n","    if print_results == True:\r\n","\r\n","        print('Top results!')\r\n","        n = 0\r\n","        for score, idx in zip(top_k[0][0], top_k[1][0]):\r\n","            print(colored(f'{n}: {idx}th', 'blue'),' corpus with score', colored( f'{score:.5f}:\\n', 'blue'),  corpora[idx])\r\n","            matches.append(idx)\r\n","            n+=1\r\n","\r\n","    return matches\r\n","\r\n","\r\n","\r\n","#--------------------------FAISS L2 distance for one example--------------------#\r\n","\r\n","def get_FAISS_L2_results(sent_embs, question, tokenizer,\r\n","                     model, max_len=512, top_n = 10,\r\n","                      print_results=True ):\r\n","    \r\n","    query_embeddings = get_query_embeddings(question, tokenizer,\r\n","                         model, max_len=512)\r\n","    \r\n","\r\n","    emb_cor = np.array([i.cpu().numpy() for i in sent_embs])\r\n","    emb_que = np.array([query_embeddings.cpu().numpy()])\r\n","\r\n","\r\n","    index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\r\n","\r\n","    index.add_with_ids(np.array(emb_cor), np.array(range(0, len(emb_cor))))\r\n","\r\n","    faiss.write_index(index, 'corona_corpora')\r\n","    index = faiss.read_index('corona_corpora')\r\n","\r\n","    top_k = index.search(emb_que, top_n)\r\n","\r\n","    matches = []\r\n","    if print_results == True:\r\n","\r\n","        n = 0\r\n","        for score, idx in zip(top_k[0][0], top_k[1][0]):\r\n","            print(colored(f'{n}: {idx}th', 'blue'),' corpus with score', colored( f'{score:.5f}:\\n', 'blue'),  corpora[idx])\r\n","            matches.append(idx)\r\n","            n+=1\r\n","\r\n","    return matches\r\n","\r\n","\r\n","    \r\n","# phrases that need to be removed from titles\r\n","corona_phrases = ['Ú©Ø±ÙˆÙ†Ø§ÛŒÛŒ', 'Ú©Ø±ÙˆÙ†Ø§Ø³Øª' ,'Ú©Ø±ÙˆÙ†Ø§', 'Ø´ÛŒÙˆØ¹', 'Ø¨Ø­Ø±Ø§Ù†', 'ÙˆÛŒØ±ÙˆØ³',\r\n","                  'ÙˆÛŒØ±ÙˆØ³ Ø¬Ø¯ÛŒØ¯', 'coronavirus', 'corona', 'Ú©ÙˆÙˆÛŒØ¯-19 ', \r\n","                  'Ú©ÙˆÙˆÛŒØ¯', 'Ø¨ÛŒÙ…Ø§Ø±ÛŒ', 'Ø¨ÛŒÙ…Ø§Ø±Ø§Ù†ÛŒ', 'Ø¨ÛŒÙ…Ø§Ø±Ø§Ù†', '-Û±Û¹', ' ÙˆÛŒ ', '19', 'Û±Û¹',\r\n","                  ' Ø¨ÛŒÙ…Ø§Ø± ', 'ÙƒØ±ÙˆÙ†Ø§', 'ÙƒÙˆÙˆÙŠØ¯', 'ÙˆÙŠØ±ÙˆØ³', r'(\\s+)',]\r\n","\r\n","\r\n","normalizer = hazm.Normalizer()\r\n","\r\n","def clean(text):\r\n","    \"\"\"Cleans the titles for the semantic models\"\"\"\r\n","    for pattern in corona_phrases:\r\n","        text = re.sub(pattern, \" \", text)\r\n","\r\n","    text = re.sub(' +[\\w] +', \" \", text)\r\n","    text = normalizer.normalize(text)\r\n","\r\n","    return text\r\n","\r\n","\r\n","#---------------------------------- get the batch results for this model-----------------------#\r\n","\r\n","def get_resutls(questions, tokenizer, model, top_n):\r\n","    \r\n","    results = []\r\n","    i=0\r\n","    for question in questions:\r\n","\r\n","        print(colored(f'{i}: ', 'blue'), question)\r\n","        # print('question type', type(question))\r\n","        i+=1\r\n","\r\n","        # we give the cleaned question to the semantic model \r\n","        question_cleaned = clean(question)\r\n","\r\n","        question_emb = get_query_embeddings(question_cleaned, tokenizer,\r\n","                         model, max_len=512)\r\n","        \r\n","        emb_que = np.array([question_emb.cpu().numpy()])\r\n","        faiss.normalize_L2(emb_que)\r\n","\r\n","        top_k = index.search(emb_que, top_n)\r\n","        indices = []\r\n","        scores = []\r\n","\r\n","        # saving all the reults in a dictionary\r\n","        for score, idx in zip(top_k[0][0], top_k[1][0]):\r\n","            \r\n","            indices.append(idx)\r\n","            scores.append(score)\r\n","\r\n","        results.append({'question':question,\r\n","                        'index':indices,\r\n","                        'score':scores})\r\n","    return results"],"execution_count":25,"outputs":[{"output_type":"stream","text":["\n","working on cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mZpvboRY2qPk"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"ennVCrsQ2go6","colab":{"base_uri":"https://localhost:8080/","height":178,"referenced_widgets":["315d400ac54d48a58867d8f3f685cc72","eb8a3cdb3b554fcab6a0e01087a9ad5f","a1161eae865e4cdb8fd6267e05cd665f","39462cbc968f4a1db23042d4bda6c334","fffc5975e52e4ed3addcc808623b8b16","9de719b26f8c49a097002eb5f031fb0b","a99e2e8959d04c8e93fa54bdd6c15403","b2e137e30d0b4502869809a050966920","053a1cbd8a284243904f836218a110dd","5ea2a0a3712d405bace574bc5cc2cf2d","ca4cfe465fea4777a61ad1b5696c9972","4c13bea958bd4ce89a46aec88df6fca6","61d9b4f1ebe84d5b90630f5c349f5b3e","e10f2d69cbe247e3b21c7137c1f66c25","586702c1d04d49aead0937842aa5e1b0","2ac75f101263429bb5214fa91aba213c","99f70fc8b40543f0a05a435a5808b427","2e743764325c488d8effc115f16bb2a8","49c7b252676c4f3ebcecbbc3a750a9f0","e23ef60224554fa6bbe8a25db4181714","7a5ed893a88e4f7aac6342b1ed8c8eff","a0ed2eb25b4349cbbcc55cf865255f7f","70e4cb3e4b34461291b4c0ade3dcd38b","1aaeafb88e7945f2a267031080cab6e3"]},"executionInfo":{"status":"ok","timestamp":1613366524109,"user_tz":-210,"elapsed":7346,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"1ff185b1-cfae-4947-98e5-9410254fc622"},"source":["albert_config = AutoConfig.from_pretrained(Albert_path)\r\n","albert_tokenizer = AutoTokenizer.from_pretrained(Albert_path)\r\n","albert_model = AutoModel.from_pretrained(Albert_path, \r\n","                                 output_hidden_states = True)"],"execution_count":8,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"315d400ac54d48a58867d8f3f685cc72","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=684.0, style=ProgressStyle(description_â€¦"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"053a1cbd8a284243904f836218a110dd","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1882978.0, style=ProgressStyle(descriptâ€¦"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"99f70fc8b40543f0a05a435a5808b427","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=73062448.0, style=ProgressStyle(descripâ€¦"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QPyaEMGo2gmx","executionInfo":{"status":"ok","timestamp":1613366529334,"user_tz":-210,"elapsed":900,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"3793b413-5861-4509-b4af-7427c7d10a4f"},"source":["albert_model.eval()\r\n","print('Model is set on the evaluation mode.')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Model is set on the evaluation mode.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"30PnKT5e2gky","executionInfo":{"status":"ok","timestamp":1613366535183,"user_tz":-210,"elapsed":3302,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"bb217c8f-021f-483e-c61a-68a022d4b9a0"},"source":["tokenized_info(corpora, albert_tokenizer, albert_config)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Total number of records: 3536\n","\u001b[31mThe maximum length: \u001b[0m 1888\n","\u001b[32mThe minimum length: \u001b[0m 11\n","\u001b[37mThe average length: \u001b[0m 135.0\n","In total  \u001b[34m98\u001b[0m  records\n","are longer than the max_len_seq wich is 512\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lF8q63Kc2giq","executionInfo":{"status":"ok","timestamp":1613366538627,"user_tz":-210,"elapsed":2605,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"5ffb8aa1-4431-4a14-93ea-2c944961a8bc"},"source":["all_inputs_albert = create_input_batches(corpora, albert_tokenizer, \r\n","                         batch_size=128, max_len=512)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:01<00:00, 16.41it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Total number of batches:  28\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"IYN9db7z2ggi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613366732387,"user_tz":-210,"elapsed":165402,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"3356f5b4-6c33-46c1-b082-eb0ac205bb93"},"source":["mean_of_all_sentences_albert = get_embeddings(all_inputs_albert, model=albert_model)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [02:34<00:00,  5.53s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","Total number of sentences:  3536\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"SxVdsjru25xd"},"source":["# Single Qustion"]},{"cell_type":"code","metadata":{"id":"WHiynr_Z2geZ"},"source":["question = 'ÙˆØ§Ú©Ø³Ù† ØªØ§ Ú†Ù‡ Ø­Ø¯ Ù…ÙˆØ«Ø± Ø§Ø³Øª'\r\n","\r\n","albert_indices = get_FAISS_cosine_results(sent_embs = mean_of_all_sentences_albert,\r\n","                         question= question,\r\n","                         tokenizer=albert_tokenizer,\r\n","                         model=albert_model,\r\n","                         max_len=512,\r\n","                         top_n = 50,\r\n","                         print_results=True )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9f2y2TEJJEh7"},"source":["# In Batches"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N6l4oiwhJGfC","executionInfo":{"status":"ok","timestamp":1613367406163,"user_tz":-210,"elapsed":3224,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}},"outputId":"3c546382-2300-400e-ea05-8631115c546c"},"source":["questions = pd.read_pickle('/content/drive/MyDrive/CoPer paper-Models/Sample Queries/Titles_with_Corona.pkl')\r\n","\r\n","\r\n","index = faiss.index_factory(768, \"Flat\", faiss.METRIC_INNER_PRODUCT)\r\n","emb_cor = np.array([i.cpu().numpy() for i in mean_of_all_sentences_albert])\r\n","faiss.normalize_L2(emb_cor)\r\n","index.add(emb_cor)\r\n","\r\n","\r\n","results = get_resutls(questions, tokenizer=albert_tokenizer, model=albert_model, top_n=50)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["\u001b[34m0: \u001b[0m Ø¯Ø±Ø¯Ø³Ø± ÙˆÚ©Ù„Ø§ Ø¨Ø§ Ú©Ø±ÙˆÙ†Ø§Ø¨Ø­Ø±Ø§Ù† Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø§Ø¯Ú¯Ø§Ù‡â€Œâ€ŒÙ‡Ø§ Ø±Ø§ ØªØ¹Ø·ÛŒÙ„ Ù…ÛŒâ€Œ Ú©Ù†Ø¯\n","\u001b[34m1: \u001b[0m Ù¢Ù¢Ù¨Ù¢ Ø§Ø¨ØªÙ„Ø§ ÛµÛ· ÙÙˆØªÛŒ Ø¬Ø¯ÛŒØ¯ Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ú©Ø´ÙˆØ± Ø®ÙˆØ²Ø³ØªØ§Ù† Ù‡Ù…Ú†Ù†Ø§Ù† Ø¯Ø± ÙˆØ¶Ø¹ÛŒØª Ù‚Ø±Ù…Ø²\n","\u001b[34m2: \u001b[0m Ø±ÙØªØ§Ø± Ø¹Ø¬ÛŒØ¨ ÙˆÛŒØ±ÙˆØ³ Ú©Ø±ÙˆÙ†Ø§ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø§ÙØ±Ø§Ø¯ÛŒ Ú©Ù‡ Ø¹Ù„Ø§Ø¦Ù… Ù†Ø¯Ø§Ø±Ù†Ø¯ Ù…Ù†ØªÙ‚Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n","\u001b[34m3: \u001b[0m Ø¨Ù‡Ø¨ÙˆØ¯ÛŒØ§ÙØªÚ¯Ø§Ù† Ú©Ø±ÙˆÙ†Ø§ Û²Û¸ Ø±ÙˆØ² Ø¨Ø¹Ø¯ Ø§Ø² Ø¨Ù‡Ø¨ÙˆØ¯ÛŒ Ù¾Ù„Ø§Ø³Ù…Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø§Ù‡Ø¯Ø§ Ú©Ù†Ù†Ø¯\n","\u001b[34m4: \u001b[0m Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯ Ú©Ø§Ù‡Ø´ÛŒ ÙÙˆØªÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø±ÙˆÙ†Ø§Ø§Ø² Ø³ÙØ± Ø¨Ù‡ Ú†Ù‡Ø§Ø± Ø§Ø³ØªØ§Ù† Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯\n","\u001b[34m5: \u001b[0m Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ú©Ø´ÙˆØ±ÛŒ ØºØ±Ø¨Ø§Ù„Ú¯Ø±ÛŒ Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ú©Ù… Ú©Ø§Ø±ÛŒ ØªÛŒØ±ÙˆØ¦ÛŒØ¯ Ø¯Ø± Ù†ÙˆØ²Ø§Ø¯Ø§Ù† Ø¯Ø± Ù¾Ø§Ù†Ø¯Ù…ÛŒ Ú©Ø±ÙˆÙ†Ø§\n","\u001b[34m6: \u001b[0m ØªØ¯Ø§ÙˆÙ… ØªØ¹Ø·ÛŒÙ„ÛŒ ØµÙ†ÙˆÙ Ù¾Ø±Ø±ÛŒØ³Ú© Ø¯Ø± Ù¾Ø§ÛŒØªØ®Øª Ù¾Ù„Ù…Ø¨ Ù‚Ù‡ÙˆÙ‡â€ŒØ®Ø§Ù†Ù‡ Ø¨Ù‡â€ŒØ®Ø§Ø·Ø± Ø¨ÛŒâ€ŒØªÙˆØ¬Ù‡ÛŒ Ø¨Ù‡ Ú©Ø±ÙˆÙ†Ø§\n","\u001b[34m7: \u001b[0m Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ø§Ø³ØªØ§Ù† Ø³Ø·Ø­ Ø¨Ø§Ù„Ø§ÛŒ Ø§Ø¨ØªÙ„Ø§ Ù…Ø±Ú¯ Ø¯Ø± Ú©Ù„ Ú©Ø´ÙˆØ±\n","\u001b[34m8: \u001b[0m Ø¯Ø± Ø§ÛŒØ±Ø§Ù† Ù…Ø·Ù„Ù‚Ø§ Ø§Ø² Ú©ÛŒØªâ€ŒÙ‡Ø§ÛŒ ØµØ§Ø¯Ø± Ø´Ø¯Ù‡ Ø¨Ù‡ Ø¢Ù„Ù…Ø§Ù† Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ú©Ø±ÙˆÙ†Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…\n","\u001b[34m9: \u001b[0m Û³Û²Û± ÙÙˆØªÛŒ Ø¬Ø¯ÛŒØ¯ Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ú©Ø´ÙˆØ±Ø´Ù…Ø§Ø± Ù‚Ø±Ø¨Ø§Ù†ÛŒØ§Ù† Ø§Ø² Ù…Ø±Ø² ÛµÛ°Ù‡Ø²Ø§Ø± ØªÙ† Ú¯Ø°Ø´Øª\n","\u001b[34m10: \u001b[0m ÙˆØ¶Ø¹ÛŒØª Ù‡Ø´Ø¯Ø§Ø± Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ú©Ø±Ù…Ø§Ù†Ø´Ø§Ù‡ Ù…Ø§Ø³Ú© Ø¨Ø²Ù†ÛŒØ¯ Ø¨ÛŒâ€ŒØ®ÛŒØ§Ù„ Ø³ÙØ± Ø´ÙˆÛŒØ¯\n","\u001b[34m11: \u001b[0m Ù¾ÛŒØ´Ø±ÙØª Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ ÙˆØ§Ú©Ø³Ù† Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ø¢Ú©Ø³ÙÙˆØ±Ø¯ Û±Û° Ù‡Ø²Ø§Ø± Ù†ÙØ± Ø¯ÛŒÚ¯Ø± Ù†ÛŒØ² ÙˆØ§Ú©Ø³ÛŒÙ†Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯\n","\u001b[34m12: \u001b[0m Ø§Ø¨ØªÙ„Ø§ÛŒ Ù†ÙØ± Ø§Ø² Ø¨ÛŒÙ…Ø§Ø±Ø§Ù† Ù†Ø§Ø¯Ø± Ø¨Ù‡ Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§Ø² Ù…Ø³Ø¦ÙˆÙ„Ø§Ù† Ø¨Ø±Ø§ÛŒ ØªØ§Ù…ÛŒÙ† Ø¯Ø§Ø±Ùˆ Ø³Ø± Ù…ÙˆÙ‚Ø¹\n","\u001b[34m13: \u001b[0m Ø³Ù†Ø¯ Ø³Ø¨Ù‚Øª Ø®Ø²Ù†Ø¯Ù‡ ØªØ±Ø³Ù†Ø§Ú© Ù…Ø±Ú¯ Ù…ÛŒØ± Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ø§ÛŒØ±Ø§Ù† Ø§Ø² Ø¬Ù‡Ø§Ù†\n","\u001b[34m14: \u001b[0m Ù…ØµØ±Ù Ø®ÙˆØ¯Ø³Ø±Ø§Ù†Ù‡â€Œ Ù…Ú©Ù…Ù„â€ŒÙ‡Ø§ ÙˆÛŒØªØ§Ù…ÛŒÙ†â€ŒÙ‡Ø§ Ù…Ù…Ù†ÙˆØ¹ Ø²Ù†Ø¬Ø¨ÛŒÙ„ Ù„ÛŒÙ…ÙˆØªØ±Ø´ Ø¯Ø± Ø¯Ø±Ù…Ø§Ù† ÙˆÛŒØ±ÙˆØ³ Ú©Ø±ÙˆÙ†Ø§ ØªØ§Ø«ÛŒØ± Ù†Ø¯Ø§Ø±Ø¯\n","\u001b[34m15: \u001b[0m Ø§Ø¹Ù…Ø§Ù„ Ù…Ø­Ø¯ÙˆÛŒØªâ€ŒÙ‡Ø§ Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ø¹Ø§Ù…Ù„ Ú©Ù†ØªØ±Ù„ Ú©Ø±ÙˆÙ†Ø§Ú©Ø¯Ø§Ù… Ø¨ÛŒÙ…Ø§Ø±Ø§Ù† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ø¨ÛŒÙ…Ø§Ø±Ø³ØªØ§Ù† Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†Ù†Ø¯\n","\u001b[34m16: \u001b[0m Ø¨Ø³ØªØ±ÛŒ Ù…Ø±Ú¯ Ù…ÛŒØ± Ù†Ø§Ø´ÛŒ Ø§Ø² Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ù‚Ù… Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú©Ø´ÙˆØ±ÛŒ\n","\u001b[34m17: \u001b[0m Ø²Ø§Ù„ÛŒ ØªØ§Ú©ÛŒØ¯ Ú©Ø±Ø¯ Ø§Ø±ØªØ¨Ø§Ø· Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ù‡ÙˆØ§ Ø¨Ø§ Ù…Ø±Ú¯ Ù…ÛŒØ± Ù†Ø§Ø´ÛŒ Ø§Ø² Ú©Ø±ÙˆÙ†Ø§\n","\u001b[34m18: \u001b[0m Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¬Ù†Ø¬Ø§Ù„ÛŒ Ø³ØªØ§Ø¯ Ù…Ù„ÛŒ Ú©Ø±ÙˆÙ†Ø§ Ø§Ø² ØªÙ„ÙˆÛŒØ²ÛŒÙˆÙ† ØªØµØ§ÙˆÛŒØ± Ù…Ø¬Ù„Ø³ Ø±Ø§ Ù¾Ø®Ø´ Ù†Ú©Ù†ÛŒØ¯ Ø¨Ø¯Ø¢Ù…ÙˆØ²ÛŒ Ø¯Ø§Ø±Ø¯\n","\u001b[34m19: \u001b[0m Ø®Ø±ÙˆØ¬ Ù…ÛŒÙ„ÛŒÙˆÙ† Ù†ÙØ± Ø§Ø² Û±Û³ Ø§Ø³ØªØ§Ù† Ú©Ø´ÙˆØ±Û²Û´Û°Û° Ù†ÙØ± Ø¹Ù„Ø§Ø¦Ù…ÛŒ Ø§Ø² Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø§Ø´ØªÙ†Ø¯\n","\u001b[34m20: \u001b[0m ÛŒÚ© Ù…Ø­Ù‚Ù‚ Ø³ÙˆØ¦Ø¯ÛŒ Ù…Ø¨ØªÙ„Ø§ÛŒØ§Ù† Ø¨Ù‡ Ú©Ø±ÙˆÙ†Ø§ Ø´Ø´ Ù…Ø§Ù‡ Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± Ø§ÛŒÙ† Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ù…ØµÙˆÙ†ÛŒØª Ø¯Ø§Ø±Ù†Ø¯\n","\u001b[34m21: \u001b[0m Ø±Ø¦ÛŒØ³ Ù…Ø¬Ù„Ø³ ÙØ±Ø¯Ø§ Ø¯Ø± Ø¬Ù„Ø³Û€ Ø³Ø±Ø§Ù† Ù‚ÙˆØ§ Ù…Ø³Ø§Ø¦Ù„ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ø±ÙˆÙ†Ø§ Ø±Ø§ Ù…Ø·Ø±Ø­ Ø®ÙˆØ§Ù‡Ù… Ú©Ø±Ø¯\n","\u001b[34m22: \u001b[0m Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ø§ÙØ²Ø§ÛŒØ´ ÙÙˆØªÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø±ÙˆÙ†Ø§ Ø¨Ø§Ø²Ú¯Ø´Ø§ÛŒÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ú†Ù‡ ØµÙˆØ±Øª Ù…ØªÙˆÙ‚Ù Ø´ÙˆØ¯\n","\u001b[34m23: \u001b[0m Ø§Ø¨ØªÙ„Ø§ÛŒ Û´Û° Ù†ÙØ± Ø¨Ù‡ Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± ÛŒÚ© Ø¹Ø±ÙˆØ³ÛŒ Ø¯Ø± Ú†Ø§Ø±Ù…Ø­Ø§Ù„ Ø¨Ø®ØªÛŒØ§Ø±ÛŒ\n","\u001b[34m24: \u001b[0m Ø¨Ù‡ Ù…Ø±Ø¯Ù… Ø¨Ú¯ÙˆÛŒÛŒØ¯ Ù‚Ø¨Ù„ Ø§Ø² Ø®Ø±ÙˆØ¬ Ø§Ø² ÙˆÙˆÙ‡Ø§Ù† ØªØ³Øª Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒÙ… Ø¯Ø± Ø³Ù„Ø§Ù…Øª Ú©Ø§Ù…Ù„ Ù‡Ø³ØªÛŒÙ…\n","\u001b[34m25: \u001b[0m Ù‚Ù„Ø¨ Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ Ø®Ø§Ø·Ø± Ú©Ø±ÙˆÙ†Ø§ Ù…ÛŒâ€ŒØ§ÛŒØ³ØªÙ†Ø¯Ù‡Ø´Ø¯Ø§Ø± Ø¨Ù‡ Ø¨ÛŒÙ…Ø§Ø±Ø§Ù† Ù‚Ù„Ø¨ÛŒ\n","\u001b[34m26: \u001b[0m Û²Û³Û¶Û¹ Ø§Ø¨ØªÙ„Ø§ Û·Ûµ ÙÙˆØªÛŒ Ø¬Ø¯ÛŒØ¯ Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ú©Ø´ÙˆØ± Ø§Ø³ØªØ§Ù†â€Œ Ø¯Ø± ÙˆØ¶Ø¹ÛŒØª Ù‚Ø±Ù…Ø²\n","\u001b[34m27: \u001b[0m ØªÙˆØ²ÛŒØ¹ Û¸Û¸Û° Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯ØªÙˆÙ…Ø§Ù† ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ ÙˆÛŒÚ˜Ù‡ Ø¯Ø± Ø¨ÛŒÙ† Ú©Ø§Ø±Ú©Ù†Ø§Ù† Ø¨Ù‡Ø¯Ø§Ø´Øª ÙˆØ¯Ø±Ù…Ø§Ù† Ø±ÙˆÙ†Ø¯ Ø§Ø¨ØªÙ„Ø§ Ø¨Ø³ØªØ±ÛŒ ÙÙˆØªÛŒ Ø¨Ø± Ø§Ø«Ø± Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ú©Ø´ÙˆØ± Ù†Ø²ÙˆÙ„ÛŒ Ø§Ø³Øª\n","\u001b[34m28: \u001b[0m Ø¯Ø§Ø±ÙˆÙ‡Ø§ÛŒ Ú©Ø±ÙˆÙ†Ø§ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ù¾Ø²Ø´Ú©Ø§Ù† Ø¨ÛŒÙ…Ø§Ø±Ø³ØªØ§Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø¯Ø§Ø±ÙˆÙØ±ÙˆØ´Ø§Ù† Ù†Ø§ØµØ±Ø®Ø³Ø±Ùˆ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡\n","\u001b[34m29: \u001b[0m Û´ÛµÛ³ ÙÙˆØªÛŒ Ø¬Ø¯ÛŒØ¯ Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ú©Ø´ÙˆØ± ÛµÛ¸Û±Û² ØªÙ† Ø¯Ø± ÙˆØ¶Ø¹ÛŒØª Ø´Ø¯ÛŒØ¯ Ø¨ÛŒÙ…Ø§Ø±ÛŒ\n","\u001b[34m30: \u001b[0m Ú†Ø±Ø§ Ø¯Ù†ÛŒØ§ Ø¨Ù‡ Ø²Ø¯Ù† Ù…Ø§Ø³Ú© Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ø¨Ù„Ù‡ Ø¨Ø§ Ú©Ø±ÙˆÙ†Ø§ ØªØ§Ú©ÛŒØ¯ Ø¯Ø§Ø±Ø¯ Û±Û° Ø¯Ù„ÛŒÙ„ Ù‚Ø§Ù†Ø¹ Ú©Ù†Ù†Ø¯Ù‡\n","\u001b[34m31: \u001b[0m Ø´Ù…Ø§Ø± Ù‚Ø±Ø¨Ø§Ù†ÛŒØ§Ù† Ø±ÙˆØ²Ø§Ù†Ù‡ Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ø§ÛŒØ±Ø§Ù† Ø¨Ù‡ Û²Û²Û± ØªÙ† Ø±Ø³ÛŒØ¯Û±Û¹ Ø§Ø³ØªØ§Ù† Ù‡Ù…Ú†Ù†Ø§Ù† Ø¯Ø± Ø´Ø±Ø§ÛŒØ· Ù‚Ø±Ù…Ø² Ù‡Ø´Ø¯Ø§Ø±\n","\u001b[34m32: \u001b[0m Ø®Ø·Ø± Ø´ÛŒÙˆØ¹ Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ù…ØªØ±Ùˆ Ø§Ø² Ù…Ø¬Ø§Ù„Ø³ Ø¹Ø±ÙˆØ³ÛŒ Ø¹Ø²Ø§ Ø¨ÛŒØ´ØªØ± Ø§Ø³Øª Ø±Ø¹Ø§ÛŒØª ÙØ§ØµÙ„Ù‡ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ù…Ù‡Ù…â€ŒØªØ± Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø§Ø³Ú©\n","\u001b[34m33: \u001b[0m Û²Û°Û° Ø¬Ø§Ù†â€ŒØ¨Ø§Ø®ØªÙ‡ Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Û²Û´ Ø³Ø§Ø¹Øª Ú¯Ø°Ø´ØªÙ‡ Û±Û¹ Ø§Ø³ØªØ§Ù† Ø¯Ø± Ø´Ø±Ø§ÛŒØ· Ù‚Ø±Ù…Ø² Ù‡Ø´Ø¯Ø§Ø±\n","\u001b[34m34: \u001b[0m Ø§ÙØ²Ø§ÛŒØ´ Ø´Ù…Ø§Ø± Ù…Ø¨ØªÙ„Ø§ÛŒØ§Ù† Ø¨Ù‡ Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ú©Ø´ÙˆØ± Ø¨Ù‡ Û²Û¹Û²Û²Ù…Ø±Ú¯ Û¹Û² Ù†ÙØ±\n","\u001b[34m35: \u001b[0m Û²ÛµÛ°Û° Ø§Ø¨ØªÙ„Ø§ Û±Û¹Û¸ ÙÙˆØªÛŒ Ø¬Ø¯ÛŒØ¯ Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ú©Ø´ÙˆØ± Ø§ÙØ²Ø§ÛŒØ´ Ù…ÙˆØ§Ø±Ø¯ Ø¨Ø³ØªØ±ÛŒ Ø¯Ø± Ø¨ÛŒÙ…Ø§Ø±Ø³ØªØ§Ù†Ù‡Ø§\n","\u001b[34m36: \u001b[0m Ø§ÙØ²Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø³Ø§ÙØ±Ø§Ù† Ø§ØªÙˆØ¨ÙˆØ³ÙÙˆØª Ø§ØªÙˆØ¨ÙˆØ³Ø±Ø§Ù† Ø¨Ø± Ø§Ø«Ø± Ø§Ø¨ØªÙ„Ø§ Ø¨Ù‡ Ú©Ø±ÙˆÙ†Ø§\n","\u001b[34m37: \u001b[0m Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ú©Ø³Ø§Ù†ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒÙ¾Ø±Ø³Ù†Ø¯ Ú©ÛŒ Ø§Ø² Ø´Ø± Ú©Ø±ÙˆÙ†Ø§ Ø®Ù„Ø§Øµ Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ…\n","\u001b[34m38: \u001b[0m Ù†Ù‡ Ù¾ÙˆÙ„ Ø®Ø±ÛŒØ¯ ÙˆØ§Ú©Ø³Ù† Ú©Ø±ÙˆÙ†Ø§ Ø±Ø§ Ø¯Ø§Ø±ÛŒÙ… Ù†Ù‡ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ ØªØ­Ø±ÛŒÙ…â€ŒÙ‡Ø§ Ø¨Ù‡ Ù…Ø§ Ù…ÛŒØ¯Ù‡Ù†Ø¯\n","\u001b[34m39: \u001b[0m Ø±Ø¯ÛŒØ§Ø¨ÛŒ ØªÙ„ÙÙ† Ù‡Ù…Ø±Ø§Ù‡ Ú©Ø±ÙˆÙ†Ø§ Ù…Ø«Ø¨ØªÙ‡Ø§ Ù†Ù‚Ø¶ Ù‚Ø±Ù†Ø·ÛŒÙ†Ù‡ Ø§Ø² Ø³ÙˆÛŒ Û·Û³ Ø¯Ø±ØµØ¯ Ù…Ø¨ØªÙ„Ø§ÛŒØ§Ù†\n","\u001b[34m40: \u001b[0m Ø±Ú©ÙˆØ±Ø¯ ØªÙ‡Ø±Ø§Ù† Ø¯Ø± ÙÙˆØªÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø±ÙˆÙ†Ø§ Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ø¹Ù…Ø§Ù„ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø¯Ø± Ø³ÙØ± Ø¨Ù‡ Ø´Ù…Ø§Ù„\n","\u001b[34m41: \u001b[0m Ù‡ÛŒÚ†â€ŒÚ©Ø³ÛŒ Ø¯Ø± Ø§ÛŒØ±Ø§Ù† Ø¨Ù‡ Ú©Ø±ÙˆÙ†Ø§ Ù…Ø¨ØªÙ„Ø§ Ù†Ø´Ø¯Ù‡ Ø­Ø§Ù„ ÛµÛ° Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒ ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù‡ Ø§Ø² Ú†ÛŒÙ† Ø®ÙˆØ¨ Ø§Ø³Øª\n","\u001b[34m42: \u001b[0m Ø§Ø¨ØªÙ„Ø§ Ø¨Ù‡ Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ø¨ÛŒÙ† Ø¬ÙˆØ§Ù†Ø§Ù† Ø¨ÛŒØ´ØªØ± Ø´Ø¯Ù‡ Ø§Ø³Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ù‚Ø§Ù‡ØªÚ¯Ø§Ù‡ Ø¨Ø±Ø§ÛŒ Ø¨ÛŒÙ…Ø§Ø±Ø§Ù†\n","\u001b[34m43: \u001b[0m Ú©Ø¯Ø§Ù… Ú¯Ø±ÙˆÙ‡Ù‡Ø§ Ø¯Ø± Ø®Ø· Ù…Ù‚Ø§Ø¨Ù„Ù‡ Ø¨Ø§ Ø¨Ø­Ø±Ø§Ù† Ú©Ø±ÙˆÙ†Ø§ Ù‡Ø³ØªÙ†Ø¯Ø®Ø· Ø´Ú©Ù†Ø§Ù† Ø±Ø§ Ø¨Ø´Ù†Ø§Ø³ÛŒÙ…\n","\u001b[34m44: \u001b[0m Ø´Ù†Ø¨Ù‡ Ø¯Ù„Ù‡Ø±Ù‡ Ø¢ÙˆØ± Ú©Ø±ÙˆÙ†Ø§ Ø§Ø² Ø±Ø§Ù‡ Ø±Ø³ÛŒØ¯Ù…Ø¨Ø§Ø¯Ø§ Ù¾Ø´ÛŒÙ…Ø§Ù† Ø´ÙˆÛŒÙ…\n","\u001b[34m45: \u001b[0m ÙÙˆØª Û±Û¸Û³ Ø¨ÛŒÙ…Ø§Ø± Ú©Ø±ÙˆÙ†Ø§ÛŒÛŒ Ø¯Ø± Ú©Ø´ÙˆØ± Ø¯Ø± Ø´Ø¨Ø§Ù†Ù‡ Ø±ÙˆØ² Ú¯Ø°Ø´ØªÙ‡ Û±Û° Ø§Ø³ØªØ§Ù† Ù‚Ø±Ù…Ø²\n","\u001b[34m46: \u001b[0m Ø§ÙØ²Ø§ÛŒØ´ Ø´Ù…Ø§Ø± Ù…Ø¨ØªÙ„Ø§ÛŒØ§Ù† Ú©Ø±ÙˆÙ†Ø§ Ø¨Ù‡ Û±Û²Û°Ù‡Ø²Ø§Ø± ØªÙ†Ù„Ø±Ø³ØªØ§Ù† Ø¯Ø± ÙˆØ¶Ø¹ÛŒØª Ù‡Ø´Ø¯Ø§Ø±Ø®ÙˆØ²Ø³ØªØ§Ù† Ù‡Ù…Ú†Ù†Ø§Ù† Ù‚Ø±Ù…Ø²\n","\u001b[34m47: \u001b[0m Ú©Ø§Ù‡Ø´ Ù…Ù‚Ø§ÙˆÙ…Øª Ø¨Ø¯Ù† Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± Ú©Ø±ÙˆÙ†Ø§ Ø¨Ø§ Ø²ÛŒØ§Ø¯Ù‡ Ø±ÙˆÛŒ Ø¯Ø± Ù…ØµØ±Ù ÙˆÛŒØªØ§Ù…ÛŒÙ†â€ŒÙ‡Ø§ Ù…Ú©Ù…Ù„â€ŒÙ‡Ø§\n","\u001b[34m48: \u001b[0m Ø¹Ø¨ÙˆØ± Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨ÛŒÙ…Ø§Ø±Ø§Ù† Ø¬Ø¯ÛŒØ¯ Ú©Ø±ÙˆÙ†Ø§ Ø§Ø² Ù…Ø±Ø² Û³Û°Û°Û° Ù†ÙØ±Û¶Û´ ØªÙ† Ø¬Ø§Ù† Ø¨Ø§Ø®ØªÙ†Ø¯\n","\u001b[34m49: \u001b[0m Ø¨ÛŒÙ…Ø§Ø±Ø§Ù† Ù¾Ø±ÙˆØ§Ù†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ú©Ø±ÙˆÙ†Ø§ Ø¯Ø± Ø§Ù…Ø§Ù† Ù…Ø§Ù†Ø¯Ù†Ø¯ Ù…ÙˆØ§Ø±Ø¯ Ø§Ø¨ØªÙ„Ø§ Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ø§Ù†Ú¯Ø´ØªØ§Ù† Ø¯Ø³Øª Ø§Ø³Øª\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pr0jCVPcJGch"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9NADhPDJGY6"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uk8v4DUC9TMU"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ot3RkgBALsBK"},"source":["# Get the results"]},{"cell_type":"code","metadata":{"id":"kvYeJ15J9TJ2","executionInfo":{"status":"ok","timestamp":1613367316111,"user_tz":-210,"elapsed":910,"user":{"displayName":"Hermione Granger","photoUrl":"","userId":"09581726699480431020"}}},"source":["with open('/content/drive/MyDrive/CoPer paper-Models/Results/AlBert.pkl', 'wb') as f:\r\n","    pickle.dump(results, f)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"3CtDaXMN9TFU"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qGfMSKAN9TDE"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBdpPSF29TA8"},"source":[],"execution_count":null,"outputs":[]}]}